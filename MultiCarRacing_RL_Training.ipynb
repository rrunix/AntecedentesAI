{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rrunix/AntecedentesAI/blob/master/MultiCarRacing_RL_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrenamiento de Agentes con Aprendizaje por Refuerzo en CarRacing-v2**\n",
    "\n",
    "---\n",
    "\n",
    "## **Descripcion**\n",
    "\n",
    "En este notebook aprenderemos a entrenar agentes de **Aprendizaje por Refuerzo (Reinforcement Learning)** utilizando el entorno **CarRacing-v2** de Gymnasium. Este entorno simula una carrera de coches donde un agente debe aprender a conducir por un circuito generado proceduralmente.\n",
    "\n",
    "### **Que es CarRacing-v2?**\n",
    "\n",
    "CarRacing-v2 es un entorno clasico de Gymnasium para aprendizaje por refuerzo. Caracteristicas principales:\n",
    "\n",
    "- **Observaciones**: Imagenes RGB de 96x96 pixeles\n",
    "- **Acciones**: Espacio continuo con 3 valores [direccion, aceleracion, freno]\n",
    "- **Recompensas**: +1000/N por cada baldosa visitada, -0.1 por cada frame\n",
    "- **Objetivo**: Completar el circuito en el menor tiempo posible\n",
    "\n",
    "### **Libreria utilizada: Stable-Baselines3**\n",
    "\n",
    "Utilizaremos **Stable-Baselines3**, una libreria de RL que proporciona implementaciones fiables y faciles de usar de algoritmos populares como PPO, A2C, SAC, etc.\n",
    "\n",
    "### **Algoritmo: PPO (Proximal Policy Optimization)**\n",
    "\n",
    "PPO es uno de los algoritmos mas populares y efectivos para tareas de control continuo. Sus ventajas incluyen:\n",
    "- Estabilidad en el entrenamiento\n",
    "- Buen rendimiento con observaciones de imagenes\n",
    "- Facil de configurar y ajustar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. Instalacion de Dependencias**\n",
    "\n",
    "Ejecuta las siguientes celdas para instalar todas las librerias necesarias. Esto incluye:\n",
    "- **Stable-Baselines3**: Libreria de algoritmos de RL\n",
    "- **Gymnasium**: Entornos de simulacion (incluye CarRacing-v2)\n",
    "- **Dependencias de visualizacion**: Para renderizar el entorno en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacion de dependencias del sistema para renderizado en Colab\n",
    "# swig es necesario para compilar box2d-py\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq -y xvfb python-opengl ffmpeg swig build-essential > /dev/null 2>&1\n",
    "print(\"Dependencias del sistema instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacion de librerias de Python\n",
    "!pip install -q swig\n",
    "!pip install -q gymnasium[box2d]\n",
    "!pip install -q stable-baselines3[extra]\n",
    "!pip install -q pyvirtualdisplay\n",
    "!pip install -q imageio imageio-ffmpeg\n",
    "print(\"Librerias de Python instaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **2. Imports y Configuracion Inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports principales\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "# Stable-Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Configuracion para visualizacion en Colab (solo si estamos en Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "    print(\"Ejecutando en Google Colab - Display virtual iniciado\")\n",
    "else:\n",
    "    print(\"Ejecutando en entorno local\")\n",
    "\n",
    "print(\"Todas las librerias cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el entorno\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INFORMACION DEL ENTORNO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Espacio de observacion: {env.observation_space}\")\n",
    "print(f\"Espacio de acciones: {env.action_space}\")\n",
    "print(f\"\\nForma de observacion: {env.observation_space.shape}\")\n",
    "print(f\"Rango de acciones: [{env.action_space.low}, {env.action_space.high}]\")\n",
    "print(\"\\nAcciones:\")\n",
    "print(\"  - Accion 0: Direccion (steering) [-1, 1]\")\n",
    "print(\"  - Accion 1: Aceleracion (gas) [0, 1]\")\n",
    "print(\"  - Accion 2: Freno (brake) [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar una observacion inicial del entorno\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(obs)\n",
    "plt.title(\"Observacion inicial del entorno\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Forma de la observacion: {obs.shape}\")\n",
    "print(f\"Tipo de datos: {obs.dtype}\")\n",
    "print(f\"Rango de valores: [{obs.min()}, {obs.max()}]\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar algunas acciones aleatorias para ver como funciona\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "frames = []\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(100):\n",
    "    # Accion aleatoria\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        frames.append(obs)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Recompensa total con acciones aleatorias: {total_reward:.2f}\")\n",
    "\n",
    "# Mostrar algunos frames\n",
    "fig, axes = plt.subplots(1, len(frames), figsize=(15, 3))\n",
    "for i, (ax, frame) in enumerate(zip(axes, frames)):\n",
    "    ax.imshow(frame)\n",
    "    ax.set_title(f\"Frame {i*20}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Secuencia de frames con acciones aleatorias\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Preparacion del Entorno para Entrenamiento**\n",
    "\n",
    "Para entrenar con Stable-Baselines3, necesitamos preparar el entorno correctamente. Usaremos:\n",
    "- **VecFrameStack**: Apila varios frames consecutivos para dar contexto temporal al agente\n",
    "- **VecTransposeImage**: Transpone las imagenes al formato esperado por PyTorch (canales primero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"Funcion para crear el entorno con configuracion correcta.\"\"\"\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "    env = Monitor(env)  # Para registrar estadisticas\n",
    "    return env\n",
    "\n",
    "# Crear entorno vectorizado\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "# Apilar 4 frames para dar contexto temporal\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Transponer imagenes para PyTorch (de HWC a CHW)\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "print(f\"Entorno preparado para entrenamiento\")\n",
    "print(f\"Forma de observacion: {env.observation_space.shape}\")\n",
    "print(f\"(4 frames apilados x 3 canales RGB = 12 canales)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **5. Configuracion del Modelo PPO**\n",
    "\n",
    "Ahora configuraremos el modelo PPO con una politica basada en redes neuronales convolucionales (CNN) para procesar las imagenes.\n",
    "\n",
    "### **Hiperparametros importantes:**\n",
    "- **learning_rate**: Tasa de aprendizaje (que tan rapido aprende el modelo)\n",
    "- **n_steps**: Pasos antes de actualizar la politica\n",
    "- **batch_size**: Tamano del lote para el entrenamiento\n",
    "- **n_epochs**: Epocas de optimizacion por actualizacion\n",
    "- **gamma**: Factor de descuento para recompensas futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion del modelo PPO\n",
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",           # Politica con CNN para imagenes\n",
    "    env=env,\n",
    "    learning_rate=3e-4,            # Tasa de aprendizaje\n",
    "    n_steps=512,                   # Pasos por actualizacion\n",
    "    batch_size=64,                 # Tamano del batch\n",
    "    n_epochs=10,                   # Epocas por actualizacion\n",
    "    gamma=0.99,                    # Factor de descuento\n",
    "    gae_lambda=0.95,               # GAE lambda para estimacion de ventaja\n",
    "    clip_range=0.2,                # Rango de clipping de PPO\n",
    "    verbose=1,                     # Mostrar progreso\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODELO PPO CONFIGURADO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Politica: CnnPolicy\")\n",
    "print(f\"Learning rate: 3e-4\")\n",
    "print(f\"Batch size: 64\")\n",
    "print(f\"Gamma (descuento): 0.99\")\n",
    "print(f\"Clip range: 0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar callbacks para guardar checkpoints durante el entrenamiento\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,                    # Guardar cada 10000 pasos\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_carracing\"\n",
    ")\n",
    "\n",
    "print(\"Callbacks configurados\")\n",
    "print(\"Los modelos se guardaran en ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **6. Entrenamiento**\n",
    "\n",
    "**Nota importante**: El entrenamiento completo puede tomar varias horas. Para este ejemplo, usaremos un numero reducido de pasos. Para obtener buenos resultados, se recomienda entrenar por al menos 1-2 millones de pasos.\n",
    "\n",
    "| Timesteps | Tiempo aproximado | Calidad esperada |\n",
    "|-----------|-------------------|------------------|\n",
    "| 50,000 | 5-10 min | Demo basica |\n",
    "| 500,000 | 1-2 horas | Resultados moderados |\n",
    "| 2,000,000 | 6-8 horas | Buenos resultados |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO\n",
    "# Ajusta total_timesteps segun el tiempo disponible\n",
    "\n",
    "TOTAL_TIMESTEPS = 50000  # Cambiar segun necesidad\n",
    "\n",
    "print(f\"Iniciando entrenamiento por {TOTAL_TIMESTEPS:,} pasos...\")\n",
    "print(\"Esto puede tomar varios minutos.\\n\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=checkpoint_callback,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_car_racing_final\")\n",
    "print(\"Modelo guardado como 'ppo_car_racing_final.zip'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **7. Evaluacion del Modelo Entrenado**\n",
    "\n",
    "Vamos a evaluar el rendimiento del agente entrenado y visualizar su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cargar un modelo guardado previamente:\n",
    "# model = PPO.load(\"ppo_car_racing_final\")\n",
    "\n",
    "def evaluate_agent(model, n_episodes=5):\n",
    "    \"\"\"Evalua el agente y retorna estadisticas.\"\"\"\n",
    "    # Crear entorno de evaluacion\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "    eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "    eval_env = VecTransposeImage(eval_env)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            total_reward += reward[0]\n",
    "            steps += 1\n",
    "            \n",
    "            if steps > 1000:  # Limite de pasos por episodio\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        print(f\"Episodio {episode + 1}: Recompensa = {total_reward:.2f}, Pasos = {steps}\")\n",
    "    \n",
    "    eval_env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RESULTADOS DE EVALUACION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Recompensa promedio: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Longitud promedio: {np.mean(episode_lengths):.0f} pasos\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Evaluar el agente\n",
    "rewards, lengths = evaluate_agent(model, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de evaluacion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(1, len(rewards) + 1), rewards, color='steelblue')\n",
    "axes[0].axhline(y=np.mean(rewards), color='red', linestyle='--', label='Promedio')\n",
    "axes[0].set_xlabel('Episodio')\n",
    "axes[0].set_ylabel('Recompensa')\n",
    "axes[0].set_title('Recompensa por Episodio')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].bar(range(1, len(lengths) + 1), lengths, color='forestgreen')\n",
    "axes[1].axhline(y=np.mean(lengths), color='red', linestyle='--', label='Promedio')\n",
    "axes[1].set_xlabel('Episodio')\n",
    "axes[1].set_ylabel('Pasos')\n",
    "axes[1].set_title('Longitud del Episodio')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabar un video del agente\n",
    "def record_video(model, video_path=\"agent_video.mp4\", max_steps=500):\n",
    "    \"\"\"Graba un video del agente jugando.\"\"\"\n",
    "    # Crear entorno para grabacion\n",
    "    video_env = DummyVecEnv([make_env])\n",
    "    video_env = VecFrameStack(video_env, n_stack=4)\n",
    "    video_env = VecTransposeImage(video_env)\n",
    "    \n",
    "    # Entorno sin procesar para obtener frames visuales\n",
    "    raw_env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    frames = []\n",
    "    obs = video_env.reset()\n",
    "    raw_obs, _ = raw_env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Renderizar frame\n",
    "        frame = raw_env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Obtener accion del modelo\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = video_env.step(action)\n",
    "        raw_obs, _, terminated, truncated, _ = raw_env.step(action[0])\n",
    "        \n",
    "        if done[0] or terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    video_env.close()\n",
    "    raw_env.close()\n",
    "    \n",
    "    # Guardar video\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    print(f\"Video guardado en: {video_path}\")\n",
    "    return video_path\n",
    "\n",
    "# Grabar video\n",
    "video_path = record_video(model, max_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar video en el notebook\n",
    "from IPython.display import Video\n",
    "Video(video_path, embed=True, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **8. Uso con Otros Entornos**\n",
    "\n",
    "Una de las grandes ventajas de **Stable-Baselines3** es que el mismo codigo puede utilizarse con muchos otros entornos de Gymnasium. A continuacion se muestran ejemplos de como adaptar este notebook para otros entornos populares.\n",
    "\n",
    "### **Entornos Clasicos de Control**\n",
    "\n",
    "Estos entornos son mas simples y entrenan mas rapido, ideales para experimentar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: CartPole-v1 (Control discreto)\n",
    "# Un pendulo que debe mantenerse en equilibrio\n",
    "\n",
    "cartpole_env = gym.make(\"CartPole-v1\")\n",
    "print(\"CartPole-v1:\")\n",
    "print(f\"  Observacion: {cartpole_env.observation_space}\")\n",
    "print(f\"  Acciones: {cartpole_env.action_space} (0=izquierda, 1=derecha)\")\n",
    "print(\"\\nComo entrenar:\")\n",
    "print('  model = PPO(\"MlpPolicy\", cartpole_env, verbose=1)')\n",
    "print('  model.learn(total_timesteps=50000)')\n",
    "\n",
    "cartpole_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 2: LunarLander-v2 (Control discreto mas complejo)\n",
    "# Aterrizar una nave espacial en la luna\n",
    "\n",
    "lunar_env = gym.make(\"LunarLander-v2\")\n",
    "print(\"LunarLander-v2:\")\n",
    "print(f\"  Observacion: {lunar_env.observation_space}\")\n",
    "print(f\"  Acciones: {lunar_env.action_space}\")\n",
    "print(\"  (0=nada, 1=motor izq, 2=motor principal, 3=motor der)\")\n",
    "print(\"\\nComo entrenar:\")\n",
    "print('  model = PPO(\"MlpPolicy\", lunar_env, verbose=1)')\n",
    "print('  model.learn(total_timesteps=500000)')\n",
    "\n",
    "lunar_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3: BipedalWalker-v3 (Control continuo)\n",
    "# Un robot bipedo que debe aprender a caminar\n",
    "\n",
    "walker_env = gym.make(\"BipedalWalker-v3\")\n",
    "print(\"BipedalWalker-v3:\")\n",
    "print(f\"  Observacion: {walker_env.observation_space}\")\n",
    "print(f\"  Acciones: {walker_env.action_space}\")\n",
    "print(\"  (4 valores continuos para controlar las articulaciones)\")\n",
    "print(\"\\nComo entrenar (requiere mas tiempo):\")\n",
    "print('  model = PPO(\"MlpPolicy\", walker_env, verbose=1)')\n",
    "print('  model.learn(total_timesteps=2000000)')\n",
    "\n",
    "walker_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entornos Atari (Observaciones de Imagenes)**\n",
    "\n",
    "Para juegos Atari, se utiliza `CnnPolicy` similar a CarRacing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 4: Juegos Atari\n",
    "# Nota: Requiere instalar ale-py y roms adicionales\n",
    "\n",
    "print(\"Para entrenar en juegos Atari:\")\n",
    "print(\"\")\n",
    "print(\"1. Instalar dependencias:\")\n",
    "print(\"   !pip install ale-py\")\n",
    "print(\"   !pip install autorom && AutoROM --accept-license\")\n",
    "print(\"\")\n",
    "print(\"2. Codigo de ejemplo para Breakout:\")\n",
    "print(\"   from stable_baselines3.common.atari_wrappers import AtariWrapper\")\n",
    "print(\"   \")\n",
    "print('   env = gym.make(\"ALE/Breakout-v5\")')\n",
    "print('   env = AtariWrapper(env)')\n",
    "print('   env = DummyVecEnv([lambda: env])')\n",
    "print('   env = VecFrameStack(env, n_stack=4)')\n",
    "print('   ')\n",
    "print('   model = PPO(\"CnnPolicy\", env, verbose=1)')\n",
    "print('   model.learn(total_timesteps=1000000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tabla de Referencia: Politicas por Tipo de Entorno**\n",
    "\n",
    "| Tipo de Entorno | Observacion | Politica | Ejemplo |\n",
    "|-----------------|-------------|----------|----------|\n",
    "| Control clasico | Vector numerico | `MlpPolicy` | CartPole, LunarLander |\n",
    "| Imagenes | RGB/Grayscale | `CnnPolicy` | Atari, CarRacing |\n",
    "| Multiples inputs | Dict/Tuple | `MultiInputPolicy` | Entornos personalizados |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Otros Algoritmos Disponibles**\n",
    "\n",
    "Ademas de PPO, Stable-Baselines3 ofrece otros algoritmos que pueden ser mas adecuados segun el problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Algoritmos disponibles en Stable-Baselines3:\")\n",
    "print(\"\")\n",
    "print(\"PPO (Proximal Policy Optimization)\")\n",
    "print(\"  - Bueno para todo tipo de problemas\")\n",
    "print(\"  - Estable y facil de configurar\")\n",
    "print('  - Uso: PPO(\"MlpPolicy\", env)')\n",
    "print(\"\")\n",
    "print(\"A2C (Advantage Actor-Critic)\")\n",
    "print(\"  - Rapido, bueno para problemas simples\")\n",
    "print(\"  - Menos estable que PPO\")\n",
    "print('  - Uso: A2C(\"MlpPolicy\", env)')\n",
    "print(\"\")\n",
    "print(\"SAC (Soft Actor-Critic)\")\n",
    "print(\"  - Excelente para control continuo\")\n",
    "print(\"  - Muy eficiente en muestras\")\n",
    "print('  - Uso: SAC(\"MlpPolicy\", env)')\n",
    "print(\"\")\n",
    "print(\"DQN (Deep Q-Network)\")\n",
    "print(\"  - Solo para acciones discretas\")\n",
    "print(\"  - Clasico de RL profundo\")\n",
    "print('  - Uso: DQN(\"MlpPolicy\", env)')\n",
    "print(\"\")\n",
    "print(\"TD3 (Twin Delayed DDPG)\")\n",
    "print(\"  - Control continuo deterministico\")\n",
    "print(\"  - Similar a SAC, puede ser mas estable\")\n",
    "print('  - Uso: TD3(\"MlpPolicy\", env)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **9. Conclusiones**\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Configurar el entorno CarRacing-v2** de Gymnasium para entrenamiento de RL\n",
    "2. **Utilizar Stable-Baselines3** para implementar el algoritmo PPO de forma sencilla\n",
    "3. **Entrenar y evaluar** un agente de aprendizaje por refuerzo\n",
    "4. **Adaptar el codigo** para otros entornos de Gymnasium\n",
    "\n",
    "### **Proximos pasos sugeridos:**\n",
    "\n",
    "- Entrenar por mas tiempo (1-2 millones de pasos) para mejores resultados\n",
    "- Experimentar con diferentes hiperparametros\n",
    "- Probar otros algoritmos como SAC o A2C\n",
    "- Usar TensorBoard para visualizar el progreso del entrenamiento:\n",
    "  ```\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir ./tensorboard_logs/\n",
    "  ```\n",
    "\n",
    "### **Recursos adicionales:**\n",
    "\n",
    "- [Documentacion de Stable-Baselines3](https://stable-baselines3.readthedocs.io/)\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "- [Tutorial de RL de OpenAI](https://spinningup.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza final\n",
    "env.close()\n",
    "print(\"Notebook completado exitosamente\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
